{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing regression parameters using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe idea behind this post is to grok the Algo behind the gradient descent using a trivial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=[0,1,2,3,4]\n",
    "Y=[1,3,7,13,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting list into pandas dataframe\n",
    "df_X = pd.DataFrame(X)\n",
    "df_Y = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQyBJjTMEktQ4QyBJjft/x7y1Q+16yrMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x263f47ddac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df_X,df_Y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(0,5)\n",
    "plt.ylim(0,25,5)\n",
    "\n",
    "#plt.axis([0, 1000, 0, 1000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to find the \"best fit\" line for the above plot. For this we are going to use  gradient descent. THe idea is to get a firm grasp of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "    The derivative of the cost for the intercept is the sum of the errors\n",
    "    The derivative of the cost for the slope is the sum of the product of the errors and the input\n",
    "    \n",
    "    RSS(w0,w1) = (yi-[w0+w1xi])2 \n",
    "    \n",
    "    taking derivate of RSS w.r.t w0 and w1 we get:\n",
    "    [yi – (w0+w1xi)]       \n",
    "    [yi – (w0+w1xi)]xi \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The algorithm\n",
    "\n",
    "Algorithm:  \n",
    "while not converged  \n",
    "w(t+1) <-- w(t) - η dg/dw \n",
    "\n",
    "1. Compute the predicted values given the current slope and intercept\n",
    "\n",
    "2. Compute the prediction errors (prediction - Y)\n",
    "\n",
    "3. Update the intercept:\n",
    "        a. compute the derivative: sum(errors)\n",
    "        b. compute the adjustment as step_size times the derivative\n",
    "        c. decrease the intercept by the adjustment\n",
    "4. Update the slope:\n",
    "       a. compute the derivative: sum(errors*input)\n",
    "       b. compute the adjustment as step_size times the derivative\n",
    "       c. decrease the slope by the adjustment\n",
    "       \n",
    "5. Compute the magnitude of the gradient\n",
    "\n",
    "6. Check for convergence \n",
    "\n",
    "  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.25 7.0\n",
      "error is 147.05441169852742\n"
     ]
    }
   ],
   "source": [
    "### FIRST STEP\n",
    "\n",
    "intercept = 0\n",
    "slope = 0\n",
    "predictions= [0,0,0,0,0]\n",
    "adjustment =0.05\n",
    "\n",
    "#computing errors\n",
    "error = [i - j for i, j in zip(predictions, Y)]\n",
    "#print(error)\n",
    "#update intercept\n",
    "intercept = intercept - adjustment * sum(error)\n",
    "\n",
    "#update slope\n",
    "derivate_slope = sum([i*j for i,j in zip(error,X)])\n",
    "slope = slope - adjustment*derivate_slope\n",
    "\n",
    "print(intercept,slope)\n",
    "error_magnitude  =((sum(error))**2 + ((derivate_slope)**2))**(0.5)\n",
    "print(\"error is =\",error_magnitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147.05441169852742"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((sum(error))**2 + ((derivate_slope)**2))**(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calcluated the slope and intercept of the line and then we measured the error which was greater then our cut off value.\n",
    "So we will iterate all the steps again . This time the initial value of slope and intercept will be same as we calclated in the FIRST STEP above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new proedcition=  [2.25, 9.25, 16.25, 23.25, 30.25]\n",
      "error 2nd pass=  [1.25, 6.25, 9.25, 10.25, 9.25]\n",
      "0.4375 2.375\n",
      "error is = 99.34944640006808\n"
     ]
    }
   ],
   "source": [
    "#Second step:\n",
    "\n",
    "intercept = 2.25\n",
    "slope = 7\n",
    "predictions = [(slope*i + intercept) for i in X]\n",
    "print(\"new proedcition= \",predictions)\n",
    "error = [i - j for i, j in zip(predictions, Y)]\n",
    "print(\"error 2nd pass= \",error)\n",
    "\n",
    "#update intercept\n",
    "intercept = intercept - adjustment * sum(error)\n",
    "#update slope\n",
    "derivate_slope = sum([i*j for i,j in zip(error,X)])\n",
    "slope = slope - adjustment*derivate_slope\n",
    "\n",
    "print(intercept,slope)\n",
    "error_magnitude  =((sum(error))**2 + ((derivate_slope)**2))**(0.5)\n",
    "print(\"error is =\",error_magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Error in Second step is greater then thresh hold value we will iterate again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new proedcition=  [0.4375, 2.8125, 5.1875, 7.5625, 9.9375]\n",
      "error 2nd pass=  [-0.5625, -0.1875, -1.8125, -5.4375, -11.0625]\n",
      "1.390625 5.59375\n",
      "error is = 67.13806320746824\n"
     ]
    }
   ],
   "source": [
    "#Third Step\n",
    "intercept = 0.4375\n",
    "slope = 2.375\n",
    "predictions = [(slope*i + intercept) for i in X]\n",
    "print(\"new proedcition= \",predictions)\n",
    "error = [i - j for i, j in zip(predictions, Y)]\n",
    "print(\"error 2nd pass= \",error)\n",
    "\n",
    "#update intercept\n",
    "intercept = intercept - adjustment * sum(error)\n",
    "#update slope\n",
    "derivate_slope = sum([i*j for i,j in zip(error,X)])\n",
    "slope = slope - adjustment*derivate_slope\n",
    "\n",
    "print(intercept,slope)\n",
    "error_magnitude  =((sum(error))**2 + ((derivate_slope)**2))**(0.5)\n",
    "print(\"error is =\",error_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope is= 4.9904183559693225\n",
      "intercept is = -0.9726848324038107\n",
      "error is = 0.04667687885208575\n"
     ]
    }
   ],
   "source": [
    "intercept = 0\n",
    "slope = 0\n",
    "adjustment =0.05\n",
    "tolerance = 0.05\n",
    "error_magnitude=10\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    predictions = [(slope*i + intercept) for i in X]\n",
    "    error = [i - j for i, j in zip(predictions, Y)]\n",
    "    #update intercept\n",
    "    intercept = intercept - adjustment * sum(error)\n",
    "    #update slope\n",
    "    derivate_slope = sum([i*j for i,j in zip(error,X)])\n",
    "    slope = slope - adjustment*derivate_slope\n",
    "    error_magnitude  =((sum(error))**2 + ((derivate_slope)**2))**(0.5)\n",
    "    if(error_magnitude < tolerance):\n",
    "        break;\n",
    "\n",
    "print(\"slope is=\",slope)\n",
    "print(\"intercept is =\",intercept) \n",
    "print(\"error is =\",error_magnitude)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_gradient_descent(tolerance,adjustment,X,Y):\n",
    "    intercept = 0\n",
    "    slope = 0\n",
    "    while True:   \n",
    "        predictions = [(slope*i + intercept) for i in X]\n",
    "        error = [i - j for i, j in zip(predictions, Y)]\n",
    "        #update intercept\n",
    "        intercept = intercept - adjustment * sum(error)\n",
    "        #update slope\n",
    "        derivate_slope = sum([i*j for i,j in zip(error,X)])\n",
    "        slope = slope - adjustment*derivate_slope\n",
    "        error_magnitude  =((sum(error))**2 + ((derivate_slope)**2))**(0.5)\n",
    "        if(error_magnitude < tolerance):\n",
    "            break;\n",
    "\n",
    "    print(\"slope is=\",slope)\n",
    "    print(\"intercept is =\",intercept) \n",
    "    print(\"error is =\",error_magnitude)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope is= 4.9904183559693225\n",
      "intercept is = -0.9726848324038107\n",
      "error is = 0.04667687885208575\n"
     ]
    }
   ],
   "source": [
    "my_gradient_descent(0.05,0.05,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope is= 4.998988731505006\n",
      "intercept is = -0.9971170949285223\n",
      "error is = 0.004926384191452688\n"
     ]
    }
   ],
   "source": [
    "my_gradient_descent(0.005,0.05,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope is= 4.999901231604713\n",
      "intercept is = -0.9997184329294512\n",
      "error is = 0.0004811492334274384\n"
     ]
    }
   ],
   "source": [
    "my_gradient_descent(0.0005,0.05,X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see with reducing error slpoe is converging towards 5 and interept towards -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yi = w0+w1 xi + εi \n",
    "\n",
    "Approach 2: Gradient descent \n",
    "\t[yi – ŷi(w0,w1)]  \n",
    "    [yi – ŷi(w0,w1)]xi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
